My last post *LINK* described a simple 'PageRank' methodology to rank football teams, using goals as 'votes' to spread team strength around the graph. At the end I highlighted a few potential improvements to take things to the next level: I will explore some of these here.

Scoring the model
In order to assess if changes to the model are improving it, it will be useful to have some metric by which to measure its success. My initial attempt at doing this will use bookmakers' odds as my 'ground truth'. The reason for this is that I believe their predictions are as good as my model is ever likely to get (currently), thus any deviance from their predictions is likely due to my model not being as powerful as their's, rather than the other way round! I will not use actual results as my 'ground truth' for now, since there is too much randomness in final scores. The bookmakers' odds are steadier.

For each match, I have bookmakers' odds for a home win, a draw, and an away win. I will convert these into percentage chances, and combine these in a weighted average to result in a score between -1 (away win 100% likely) and +1 (home win 100% likely). To illustrate with an example, for Tottenham vs. Aston Villa on 14th May 2014, we had the following odds:

Home win (Tottenham) = 3/2
Draw = 9/2
Away win (Aston Villa) = 15/2

Converting to percentages, these imply a 67% chance of a home win, 22% chance of a draw, and a 13% chance of an away win. Note how the odds add up to over 100% - this is how the bookmaker weights the odds in its favour.

Thus combining these as ( (0.67 x 1) + (0.22 x 0) + (0.13 x -1) ) / 1.02, we get 0.52. On our scale of -1 to 1, this looks like something between a draw and a home win is most likely. The reverse fixture, by the way, with Aston Villa playing at home, leads to a bookmaker's combined score of -0.23, suggesting a draw is more likely, or possibly an away win.

IMAGE HERE

So, we have a bookmaker's prediction for each game - how will we compare our own scorerank model output with this? We must ensure that the model only uses *previous*, historical, games to make its prediction (i.e. no games from the future!), and ultimately outputs one number. For now, I will use the difference in scorerank between the two teams, for a graph constructed using one season's worth of historical games. The scorerank differences (home team scorerank minus away team scorerank) can then be correlated with the bookmaker's combined value we have previously calculated. Note that there is no reason for these values to be the *same*, but we do expect a *correlation* between many pairs of values. Therefore, the R^2 coefficient of determination will be my accuracy measure.

Results
To align with my previous blog, I have considered the 2013-2014 Premier League season only, for now. For each match, I look back over the previous 380 Premier League games on record (i.e. the previous season's-worth of games, so these stretch back into 2012-2013) and build a ScoreRank model using this data only. This ScoreRank model is then used to determine the difference in strength between the two teams in question, and this value is stored along with the normalised bookmaker prediction (as described above).

IMAGE

The ScoreRank vs. Bookmaker pairs are collected for the whole of the 2013-2014 season, and these values are plotted below. The R^2 coefficient is 0.82 - not a bad correlation! From here-on, any work I do on this can be evaluated by how this chart, and the R^2 value, are altered.

IMAGE