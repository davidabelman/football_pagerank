Modelling football team strength using PageRank

A long time ago I tried to make an app to predict football match results, to see how my comparisons would compare with the bookies. The previous 'X' results (including their goals, shots on target, etc.) for each team was a strong predictor - but something I found hard to model at the time was a purer measure of 'team strength'.

I've decided to revisit this problem and use a PageRank-like algorithm (*LINK*) in my approach. Similar work has been carried out *here* (http://www2.sas.com/proceedings/forum2008/151-2008.pdf) for NFL teams, and doubtless in various other places too. This type of work can be useful in particular when teams from separate leagues play one another (for example in a cup competition) - in this case, the teams are unlikely to have met before, whilst each team won't have played similar opponents recently either: it's therefore difficult to compare their strength directly. However, by joining all teams in a graph structure, we can potentially calibrate the strength of two leagues based on the small number of teams who have played one another in recent history.

IMAGE HERE

Approach 1: Goals as votes
Taking inspiration from the NFL work, my first attempt will count each goal scored by Team A against Team B as a 'vote' from Team B to Team A, in acknowledgement of the goal. The 'pagerank' (which I will call 'scorerank' for clarity from here-on) for the graph is intialised equally across all teams at first. It is then iteratively distributed around the graph according to these 'votes' (i.e. goals). Thus a team who has conceded 50 goals over the season will send its scorerank out in 50 'votes' proportioned to the other teams according to how many goals were scored against them. It will also receive votes from the other teams according to how many goals it scored against them. On top of this, a fraction of the scorerank is randomly distributed across the graph at each iteration (see original *LINK* paper for details).

To start things off simply, I have used data from the 2013-2014 premier league season. A graph of 20 teams has been created, and all goals scored across all 380 games have been added as (weighted) edges to the graph between teams. The algorithm then iterates over this.

Firstly, let's visualise the final graph (after 10 iterations) to provide a better idea of what's going on. The sizes of the nodes represent how much scorerank - or team strength - has been attributed to each team, based on its incoming scorerank (i.e. goals scored) and outgoing scorerank (i.e. goals conceded). The colours also represent team strength. The edges between the nodes represent the 'movement' of scorerank / team strength, with the heavier lines (with arrow-heads) showing where more goals have been scored. Looking at Everton for example (towards the bottom of the figure) we can see that they scored lots of goals against Fulham (7 goals in 2 games) but conceded lots against Liverpool (also 7 goals in 2 games). We can see that Manchester City in particular scored lots of goals in the 2013-2014 season, with many heavy arrow-heads pointing towards it. Their scorerank reflects this (seen in the size of the node), as does the fact that they were the overall champions (with Liverpool a close second).

IMAGE HERE

How can we evaluate how well the algorithm performs? A first port of call could be the league table (they do say that 'the league table never lies', dubiously). The following chart shows the relative ranking for the scorerank model versus the final premier league table.

IMAGE HERE

The scorerank model accurately portrays the top five teams in the table. Below this there is a reasonably strong correlation between scorerank and final league position, though not a perfect alignment. The current algorithm suggests that Norwich, Hull and Crystal Palace should have been relegated (lowest scorerank values along the y-axis) whereas in reality it was Norwich, Fulham and Cardiff that were relegated.

Using this figure it is hard to say whether scorerank reflects team quality any better than the final league table does - perhaps a better way to measure success would be to use the model (built on historical data) to predict score outcomes (from future games). The premier league table could be used as an alternative 'model' to predict score outcomes, and we can compare which methodology performs better.

As a final thought on this initial iteration of the algorithm, let's look at how the scorerank converges. The graph initially starts out with a scorerank of 

Ideas:
Limit scorerank to 1 per game (helps 0-0 draws be shown for good/bad pairing)
Split home vs away as separate nodes
Other metrics than goals - shots on target? Close/far shots?
Treat attack and defence differently (same graph, AttackRank, DefenceRank)
Dampen effects over time
Success metric - predicting results (tune parameters using this)

